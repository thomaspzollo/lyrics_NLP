# -*- coding: utf-8 -*-
"""genius.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yQNchguKWrhekIFYuO0wYU1Mns5o3P_4
"""

import pandas as pd
import numpy as np
from numpy import mean
from numpy import std

import csv
import sys
import string
import re
import os

import matplotlib.pyplot as plt

from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score
from sklearn.metrics import roc_auc_score

from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler 
from sklearn import linear_model
from sklearn.linear_model import LogisticRegression
from sklearn import model_selection
from sklearn import metrics
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold

import torch
from torch import nn
from torch import tensor
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler

from sentence_transformers import SentenceTransformer

import seaborn as sns
from tqdm.notebook import tqdm

sbert = SentenceTransformer('stsb-roberta-base')

# ## Multiclass
# artists = ["Eminem","DaBaby","Travis Scott",'Nicki Minaj']
# train_df = pd.read_csv('/content/train_df_m.csv')
# val_df = pd.read_csv('/content/val_df_m.csv')
# test_df = pd.read_csv('/content/test_df_m.csv')

# ## Binary
# # artists = ["Eminem","DaBaby"]
# # train_df = pd.read_csv('/content/train_df_b.csv')
# # val_df = pd.read_csv('/content/val_df_b.csv')
# # test_df = pd.read_csv('/content/test_df_b.csv')

## Multiclass Unbalanced
artists = ["Eminem","DaBaby","Travis Scott","Drake"]
train_df = pd.read_csv('/content/drive/My Drive/ML_proj/train_df_m_unbalanced.csv')
val_df = pd.read_csv('/content/drive/My Drive/ML_proj/val_df_m_unbalanced.csv')
test_df = pd.read_csv('/content/drive/My Drive/ML_proj/test_df_m_unbalanced.csv')

art2idx = {}
n_artists = 0

for artist in artists:

  art2idx[artist] = n_artists
  n_artists += 1

print(art2idx)

print(train_df)
print(val_df)
print(test_df)

def myMap(n):
    return str(n)

train_text = train_df['Lyric'].to_list()
train_text = list(map(myMap, train_text))
train_embeds = sbert.encode(train_text)
X_train = train_embeds = np.array(train_embeds)

val_text = val_df['Lyric'].to_list()
val_text = list(map(myMap, val_text))
val_embeds = sbert.encode(val_text)
X_val = val_embeds = np.array(val_embeds)

test_text = test_df['Lyric'].to_list()
test_text = list(map(myMap, test_text))
test_embeds = sbert.encode(test_text)
X_test = test_embeds = np.array(test_embeds)

train_y = train_df['Artist'].to_list()
train_y = [art2idx[x] for x in train_y]
y_train = train_y = np.asarray(train_y)

val_y = val_df['Artist'].to_list()
val_y = [art2idx[x] for x in val_y]
y_val = val_y = np.asarray(val_y)

test_y = test_df['Artist'].to_list()
test_y = [art2idx[x] for x in test_y]
y_test = test_y = np.asarray(test_y)

print(X_train.shape)
print(train_y.shape)

print(X_val.shape)
print(val_y.shape)

print(X_test.shape)
print(test_y.shape)

word_embed_size = train_embeds.shape[1]
print(word_embed_size)

# arrays = [X_train,X_test,X_val]
# X_all = np.concatenate(arrays, axis=0)

### Multi-class
model = LogisticRegression(multi_class='ovr', solver='liblinear')
### Binary
# model = LogisticRegression(solver='liblinear')

model.fit(X_train, y_train)

print('n_artists = ' + str(n_artists) + '\n')

score = model.score(X_train,y_train)
print('train ' + str(score))
score = model.score(X_val,y_val)
print('val ' + str(score))
score = model.score(X_test,y_test)
print('test ' + str(score))

### Uncomment for multi-class
print(roc_auc_score(y_train, model.predict_proba(X_train), multi_class='ovr'))

fig, ax = plt.subplots(figsize=(10, 6))
ax.set_title('Confusion Matrx')

disp = metrics.plot_confusion_matrix(model, X_train, y_train, display_labels=art2idx.keys(), ax = ax)
disp.confusion_matrix

print('Train')
print(metrics.classification_report(y_train, model.predict(X_train), digits=3))

fig, ax = plt.subplots(figsize=(10, 6))
ax.set_title('Confusion Matrx')

disp = metrics.plot_confusion_matrix(model, X_val, y_val, display_labels=art2idx.keys(), ax = ax)
disp.confusion_matrix

print('Val')
print(metrics.classification_report(y_val, model.predict(X_val), digits=3))

class ClassifierDataset(Dataset):
    
    def __init__(self, X_data, y_data):
        self.X_data = X_data
        self.y_data = y_data
        
    def __getitem__(self, index):
        return self.X_data[index], self.y_data[index]
        
    def __len__ (self):
        return len(self.X_data)

scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)
X_test = scaler.transform(X_test)
X_train, y_train = np.array(X_train), np.array(y_train)
X_val, y_val = np.array(X_val), np.array(y_val)
X_test, y_test = np.array(X_test), np.array(y_test)

train_dataset = ClassifierDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long())
val_dataset = ClassifierDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).long())
test_dataset = ClassifierDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).long())

EPOCHS = 200
BATCH_SIZE = 16
LEARNING_RATE = 0.0005
NUM_FEATURES = word_embed_size
NUM_CLASSES = n_artists

train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(dataset=val_dataset, batch_size=1)
test_loader = DataLoader(dataset=test_dataset, batch_size=1)

class MulticlassClassification(nn.Module):
    def __init__(self, num_feature, num_class):
        super(MulticlassClassification, self).__init__()
        
        hidden_size = num_feature

        # self.proj_layer = nn.Linear(num_feature, hidden_size)
        self.layer_1 = nn.Linear(num_feature, hidden_size)
        self.layer_2 = nn.Linear(hidden_size, hidden_size)
        self.layer_3 = nn.Linear(hidden_size, 64)
        self.layer_out = nn.Linear(64, num_class)
        
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(p=0.2)
        self.batchnorm1 = nn.BatchNorm1d(hidden_size)
        self.batchnorm2 = nn.BatchNorm1d(hidden_size)
        self.batchnorm3 = nn.BatchNorm1d(64)
        
    def forward(self, x):

        residual = x
        x = self.layer_1(x)
        x = residual + x
        x = self.batchnorm1(x)
        x = self.relu(x)
        
        residual = x
        x = self.layer_2(x)
        x = residual + x
        x = self.batchnorm2(x)
        x = self.relu(x)
        x = self.dropout(x)
        
        x = self.layer_3(x)
        x = self.batchnorm3(x)
        x = self.relu(x)
        x = self.dropout(x)
        
        x = self.layer_out(x)
        
        return x

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print("device = " + str(device))

nn_model = MulticlassClassification(num_feature = NUM_FEATURES, num_class=NUM_CLASSES)
nn_model.to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(nn_model.parameters(), lr=LEARNING_RATE)
print(nn_model)

def multi_acc(y_pred, y_test):
    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)
    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    
    
    correct_pred = (y_pred_tags == y_test).float()
    acc = correct_pred.sum() / len(correct_pred)
    
    acc = torch.round(acc * 100)
    
    return acc

accuracy_stats = {
    'train': [],
    "val": []
}
loss_stats = {
    'train': [],
    "val": []
}

print("Begin training.")
for e in tqdm(range(1, EPOCHS+1)):
    
    # TRAINING
    train_epoch_loss = 0
    train_epoch_acc = 0
    nn_model.train()
    for X_train_batch, y_train_batch in train_loader:
        X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)
        optimizer.zero_grad()
        
        y_train_pred = nn_model(X_train_batch)
        
        train_loss = criterion(y_train_pred, y_train_batch)
        train_acc = multi_acc(y_train_pred, y_train_batch)
        
        train_loss.backward()
        optimizer.step()
        
        train_epoch_loss += train_loss.item()
        train_epoch_acc += train_acc.item()
        
        
    # VALIDATION    
    with torch.no_grad():
        
        val_epoch_loss = 0
        val_epoch_acc = 0
        
        nn_model.eval()
        for X_val_batch, y_val_batch in val_loader:
            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)
            
            y_val_pred = nn_model(X_val_batch)
                        
            val_loss = criterion(y_val_pred, y_val_batch)
            val_acc = multi_acc(y_val_pred, y_val_batch)
            
            val_epoch_loss += val_loss.item()
            val_epoch_acc += val_acc.item()

    loss_stats['train'].append(train_epoch_loss/len(train_loader))
    loss_stats['val'].append(val_epoch_loss/len(val_loader))
    accuracy_stats['train'].append(train_epoch_acc/len(train_loader))
    accuracy_stats['val'].append(val_epoch_acc/len(val_loader))
                              
    print(f'Epoch {e+0:03}: | Train Loss: {train_epoch_loss/len(train_loader):.5f} | Val Loss: {val_epoch_loss/len(val_loader):.5f} | Train Acc: {train_epoch_acc/len(train_loader):.3f}| Val Acc: {val_epoch_acc/len(val_loader):.3f}')

# Create dataframes
train_val_acc_df = pd.DataFrame.from_dict(accuracy_stats).reset_index().melt(id_vars=['index']).rename(columns={"index":"epochs"})
train_val_loss_df = pd.DataFrame.from_dict(loss_stats).reset_index().melt(id_vars=['index']).rename(columns={"index":"epochs"})
# Plot the dataframes
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,7))
sns.lineplot(data=train_val_acc_df, x = "epochs", y="value", hue="variable",  ax=axes[0]).set_title('Train-Val Accuracy/Epoch')
sns.lineplot(data=train_val_loss_df, x = "epochs", y="value", hue="variable", ax=axes[1]).set_title('Train-Val Loss/Epoch')

y_pred_list = []
with torch.no_grad():
    nn_model.eval()
    for X_batch, _ in test_loader:
        X_batch = X_batch.to(device)
        y_test_pred = nn_model(X_batch)
        _, y_pred_tags = torch.max(y_test_pred, dim = 1)
        y_pred_list.append(y_pred_tags.cpu().numpy())
y_pred_list = [a.squeeze().tolist() for a in y_pred_list]

print( accuracy_score(y_pred_list,y_test) )
# print( f1_score(y_pred_list,y_test) )
x_axis_labels = y_axis_labels = list(art2idx.keys())
print(metrics.classification_report(y_test, y_pred_list, digits=3))

confusion_matrix_df = pd.DataFrame(confusion_matrix(y_test, y_pred_list))

sns.heatmap(confusion_matrix_df, annot=True, xticklabels=x_axis_labels, yticklabels=y_axis_labels)

